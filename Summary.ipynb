{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f40ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is demonstrating various Natural Language Processing techniques we've learned so far in this class.\n",
      "\n",
      "I got a small sample of some of my friend Conor's poetry, taken from some portfolio submission he did a bit ago. It's 8 pages total, though I had to skip the 7th since it's doesn't have any real text on it.\n",
      "\n",
      "I read the text from a PDF directly via pypdf, including the titles for each poem & the name at the start, and then do a little immediate clean-up for some of the punctuation. It's mostly just putting periods at the end of stanzas without punctuation, since otherwise the tokenizer gets a little confused on what to break on - I didn't have to do this, but it seemed more conducive to interesting data.\n",
      "\n",
      "After, the sentences are tokenized and some basic exploratory processing is done. Following that, the text is cleaned up - non-alpha words are purged, all words are cast to lowercase, etc.\n",
      "\n",
      "Then, TF-IDF vectorization is done to get some of the most \"important\" sentences in the text - the top 3 are printed. It's a little biased towards longer sentences but it's not like the text isn't full of some of those.\n",
      "\n",
      "Following the TF-IDF, less technical things are done - a WordCloud on both the original and cleaned up text, translating the original text to spanish, using displacy to identify important words in the original/cleaned up text, and then finally POS tagging & FreqDist counting of words in the original/cleaned up texts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "This is demonstrating various Natural Language Processing techniques we've learned so far in this class.\n",
    "\n",
    "I got a small sample of some of my friend Conor's poetry, taken from some portfolio submission he did a bit ago. It's 8 pages total, though I had to skip the 7th since it's doesn't have any real text on it.\n",
    "\n",
    "I read the text from a PDF directly via pypdf, including the titles for each poem & the name at the start, and then do a little immediate clean-up for some of the punctuation. It's mostly just putting periods at the end of stanzas without punctuation, since otherwise the tokenizer gets a little confused on what to break on - I didn't have to do this, but it seemed more conducive to interesting data.\n",
    "\n",
    "After, the sentences are tokenized and some basic exploratory processing is done. Following that, the text is cleaned up - non-alpha words are purged, all words are cast to lowercase, etc.\n",
    "\n",
    "Then, TF-IDF vectorization is done to get some of the most \"important\" sentences in the text - the top 3 are printed. It's a little biased towards longer sentences but it's not like the text isn't full of some of those.\n",
    "\n",
    "Following the TF-IDF, less technical things are done - a WordCloud on both the original and cleaned up text, translating the original text to spanish, using displacy to identify important words in the original/cleaned up text, and then finally POS tagging & FreqDist counting of words in the original/cleaned up texts.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dac672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Overall, this was pretty interesting data to look at. I was curious to see which words were most common here and what that says about the poetry, but also curious from a technical standpoint what happens with really long 'sentences' like in Conor's poetry.\n",
      "\n",
      "It's especially interesting the things that spacy highlights - like \"grey forecast\" as a person, for some reason, which is obviously not right. I guess it doens't really like very figurative/flowery language.\n",
      "\n",
      "The WordClouds were also pretty fun to look at - I didn't realize \"back\" was such a common word in general let alone in this poetry.\n",
      "\n",
      "One downside I did notice - TF-IDF seems very awkward for very varying sentence lengths. It really liked the longest sentences, presumably since they share a lot of similarities etc. with the other ones, but that's something to consider when using TF-IDF with future datasets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "Overall, this was pretty interesting data to look at. I was curious to see which words were most common here and what that says about the poetry, but also curious from a technical standpoint what happens with really long 'sentences' like in Conor's poetry.\n",
    "\n",
    "It's especially interesting the things that spacy highlights - like \"grey forecast\" as a person, for some reason, which is obviously not right. I guess it doens't really like very figurative/flowery language.\n",
    "\n",
    "The WordClouds were also pretty fun to look at - I didn't realize \"back\" was such a common word in general let alone in this poetry.\n",
    "\n",
    "One downside I did notice - TF-IDF seems very awkward for very varying sentence lengths. It really liked the longest sentences, presumably since they share a lot of similarities etc. with the other ones, but that's something to consider when using TF-IDF with future datasets.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c1a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
